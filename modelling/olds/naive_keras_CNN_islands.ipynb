{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Very naive deep learning on the vector of surrounding bases\n",
    "\n",
    "---\n",
    "\n",
    "### Data\n",
    "\n",
    "Naive feature vectors. The original sequence of validation/test and train data does not overlap! ( but train data points can overlap with train data points, and test-validation can overlap with test-validation data ) This overlapping does not lead to unintentional label leakage!\n",
    "\n",
    "\n",
    "\n",
    "### Notes\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instruct theano to use gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 670 (CNMeM is disabled, CuDNN not available)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['THEANO_FLAGS']='device=gpu'\n",
    "\n",
    "import sys\n",
    "sys.path.append('../my_modules')\n",
    "from loading_utils import read_my_data\n",
    "\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os,subprocess\n",
    "workdir='/mnt/Data1/ribli/methylation_code/modelling'\n",
    "subprocess.call(['mkdir',workdir])\n",
    "os.chdir(workdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data... \n",
      "Loading data... \n"
     ]
    }
   ],
   "source": [
    "train_id,train_x,train_y = read_my_data(\n",
    "    fname='../prepare_data/big_train_feat_vect.csv')\n",
    "test_id,test_x,test_y = read_my_data(\n",
    "    fname='../prepare_data/big_test_feat_vect.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "annot=pd.read_csv('../explore_data/relevant_annotations.csv',sep='\\t',header=None)\n",
    "annot.columns=['id','Regulatory_Feature_Group','Relation_to_UCSC_CpG_Island',\n",
    "    'Strand','Infinium_Design_Type','Random_Loci','Methyl27_Loci']\n",
    "annot.fillna(0,inplace=True)\n",
    "train_merged=pd.DataFrame(train_id,columns=['id']).merge(annot,on=['id'])\n",
    "test_merged=pd.DataFrame(test_id,columns=['id']).merge(annot,on=['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select inidces for islands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cg_exl_idx=np.array([x[499]==2 and x[500]==3 for x in train_x])\n",
    "annot_idx=np.array(np.zeros(len(train_x)),dtype=bool)\n",
    "annot_idx[train_merged[train_merged.Relation_to_UCSC_CpG_Island=='Island'].index]=True\n",
    "train_idx=cg_exl_idx & annot_idx\n",
    "train_idx_0=cg_exl_idx & annot_idx & (train_y ==0)\n",
    "train_idx_1=cg_exl_idx & annot_idx & (train_y ==1)\n",
    "\n",
    "\n",
    "cg_exl_idx=np.array([x[499]==2 and x[500]==3 for x in test_x])\n",
    "annot_idx=np.array(np.zeros(len(test_x)),dtype=bool)\n",
    "annot_idx[test_merged[test_merged.Relation_to_UCSC_CpG_Island=='Island'].index]=True\n",
    "test_idx=cg_exl_idx & annot_idx\n",
    "test_idx_0=cg_exl_idx & annot_idx & (test_y==0)\n",
    "test_idx_1=cg_exl_idx & annot_idx & (test_y==1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape x data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make it image like\n",
    "train_x,test_x=[x.reshape((-1,1,1000,1)) for x in (train_x,test_x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "/mnt/Data1/ribli/tools/anaconda/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:5: UserWarning: downsample module has been moved to the pool module.\n",
      "  warnings.warn(\"downsample module has been moved to the pool module.\")\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D,MaxPooling2D\n",
    "\n",
    "input_dim=train_x.shape[2]\n",
    "activation='relu'\n",
    "loss='binary_crossentropy'\n",
    "optimizer='adadelta'\n",
    "init='uniform'\n",
    "pool_size=(8,1)\n",
    "window_size=5\n",
    "dense_n=64\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#Convolution layer 1\n",
    "model.add(Convolution2D(20,window_size,1, border_mode='valid',input_shape=(1,input_dim,1)))\n",
    "model.add(Activation(activation))\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "#model.add(Dropout(0.25))\n",
    "\n",
    "#Convolution layer 2\n",
    "model.add(Convolution2D(50,window_size,1, border_mode='valid'))\n",
    "model.add(Activation(activation))\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "#model.add(Dropout(0.25))\n",
    "\n",
    "#Dense layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(dense_n,activation=activation))\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "#final layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#compile model\n",
    "model.compile(loss=loss,optimizer=optimizer,class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "\n",
    "def fit_keras_model(model,train_x,train_y,test_x,test_y,validation_split=0.05):\n",
    "    start=time.time()\n",
    "    \n",
    "    #callbacks\n",
    "    best_model=ModelCheckpoint('best_model',save_best_only=True,verbose=1)\n",
    "    early_stop=EarlyStopping(patience=7,verbose=1)\n",
    "    \n",
    "    #train it\n",
    "    callb_hist=model.fit(train_x,train_y,nb_epoch = 100,\n",
    "                         show_accuracy=True,verbose=1,\n",
    "                        validation_split=validation_split,\n",
    "                        callbacks=[best_model,early_stop])\n",
    "    #predict\n",
    "    model.load_weights('best_model')\n",
    "    train_pred=model.predict_classes(train_x).ravel()\n",
    "    test_pred=model.predict_classes(test_x).ravel()\n",
    "\n",
    "    #check errors\n",
    "    print 'train score:',list((train_pred==train_y)).count(True)/float(len(train_y))\n",
    "    print 'test score:',list((test_pred==test_y)).count(True)/float(len(test_y))\n",
    "\n",
    "    print 'It took:',time.time()-start    \n",
    "    return train_pred,test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 81434 samples, validate on 4287 samples\n",
      "Epoch 1/100\n",
      "81434/81434 [==============================] - 31s - loss: 0.3580 - acc: 0.8708 - val_loss: 0.3603 - val_acc: 0.8729\n",
      "Epoch 00000: val_loss improved from inf to 0.36029, saving model to best_model\n",
      "Epoch 2/100\n",
      "81434/81434 [==============================] - 31s - loss: 0.3152 - acc: 0.8723 - val_loss: 0.2945 - val_acc: 0.8733\n",
      "Epoch 00001: val_loss improved from 0.36029 to 0.29447, saving model to best_model\n",
      "Epoch 3/100\n",
      "81434/81434 [==============================] - 31s - loss: 0.2897 - acc: 0.8831 - val_loss: 0.2900 - val_acc: 0.8927\n",
      "Epoch 00002: val_loss improved from 0.29447 to 0.28997, saving model to best_model\n",
      "Epoch 4/100\n",
      "81434/81434 [==============================] - 31s - loss: 0.2702 - acc: 0.8930 - val_loss: 0.3270 - val_acc: 0.8726\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 5/100\n",
      "81434/81434 [==============================] - 31s - loss: 0.2541 - acc: 0.9009 - val_loss: 0.2669 - val_acc: 0.8981\n",
      "Epoch 00004: val_loss improved from 0.28997 to 0.26687, saving model to best_model\n",
      "Epoch 6/100\n",
      "81434/81434 [==============================] - 31s - loss: 0.2474 - acc: 0.9047 - val_loss: 0.5302 - val_acc: 0.7362\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 7/100\n",
      "81434/81434 [==============================] - 31s - loss: 0.2383 - acc: 0.9067 - val_loss: 0.2585 - val_acc: 0.8936\n",
      "Epoch 00006: val_loss improved from 0.26687 to 0.25854, saving model to best_model\n",
      "Epoch 8/100\n",
      "81434/81434 [==============================] - 31s - loss: 0.2308 - acc: 0.9120 - val_loss: 0.2158 - val_acc: 0.9223\n",
      "Epoch 00007: val_loss improved from 0.25854 to 0.21578, saving model to best_model\n",
      "Epoch 9/100\n",
      "81434/81434 [==============================] - 31s - loss: 0.2262 - acc: 0.9135 - val_loss: 0.2718 - val_acc: 0.8911\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 10/100\n",
      "81434/81434 [==============================] - 31s - loss: 0.2149 - acc: 0.9183 - val_loss: 0.2383 - val_acc: 0.9139\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 11/100\n",
      "81434/81434 [==============================] - 31s - loss: 0.2075 - acc: 0.9205 - val_loss: 0.2058 - val_acc: 0.9268\n",
      "Epoch 00010: val_loss improved from 0.21578 to 0.20582, saving model to best_model\n",
      "Epoch 12/100\n",
      "81434/81434 [==============================] - 31s - loss: 0.2016 - acc: 0.9227 - val_loss: 0.2044 - val_acc: 0.9270\n",
      "Epoch 00011: val_loss improved from 0.20582 to 0.20443, saving model to best_model\n",
      "Epoch 13/100\n",
      "81434/81434 [==============================] - 31s - loss: 0.1907 - acc: 0.9276 - val_loss: 0.2134 - val_acc: 0.9214\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 14/100\n",
      "81434/81434 [==============================] - 31s - loss: 0.1798 - acc: 0.9327 - val_loss: 0.3077 - val_acc: 0.8901\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 15/100\n",
      "81434/81434 [==============================] - 31s - loss: 0.1697 - acc: 0.9370 - val_loss: 0.2241 - val_acc: 0.9219\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 16/100\n",
      "81434/81434 [==============================] - 31s - loss: 0.1592 - acc: 0.9405 - val_loss: 0.3029 - val_acc: 0.9013\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 17/100\n",
      "81434/81434 [==============================] - 31s - loss: 0.1512 - acc: 0.9429 - val_loss: 0.2332 - val_acc: 0.9179\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 18/100\n",
      "81434/81434 [==============================] - 31s - loss: 0.1380 - acc: 0.9484 - val_loss: 0.2443 - val_acc: 0.9177\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 19/100\n",
      "81434/81434 [==============================] - 31s - loss: 0.1277 - acc: 0.9527 - val_loss: 0.4024 - val_acc: 0.8456\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 20/100\n",
      "81434/81434 [==============================] - 31s - loss: 0.1168 - acc: 0.9566 - val_loss: 0.3553 - val_acc: 0.8988\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 00019: early stopping\n",
      "85721/85721 [==============================] - 14s    \n",
      "3932/3932 [==============================] - 0s     \n",
      "train score: 0.94149624946\n",
      "test score: 0.913784333672\n",
      "It took: 641.489870071\n"
     ]
    }
   ],
   "source": [
    "N_train,N_test=train_x.shape[0],test_x.shape[0]\n",
    "train_pred,test_pred=fit_keras_model(\n",
    "    model,train_x[train_idx],train_y[train_idx],test_x[test_idx],test_y[test_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "data and prediction balance:\n",
      "0.871303414566\n",
      "0.871566632757\n",
      "0.900479462442\n",
      "0.903865717192\n"
     ]
    }
   ],
   "source": [
    "print '\\ndata and prediction balance:'\n",
    "print 1-np.mean(train_y[train_idx])\n",
    "print 1-np.mean(test_y[test_idx])\n",
    "\n",
    "print 1-np.mean(train_pred)\n",
    "print 1-np.mean(test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "bal_train_x=np.concatenate([train_x[train_idx_0][:np.sum(train_idx_1)],train_x[train_idx_1]])\n",
    "bal_train_y=np.concatenate([train_y[train_idx_0][:np.sum(train_idx_1)],train_y[train_idx_1]])\n",
    "\n",
    "bal_test_x=np.concatenate([test_x[test_idx_0][:np.sum(test_idx_1)],test_x[test_idx_1]])\n",
    "bal_test_y=np.concatenate([test_y[test_idx_0][:np.sum(test_idx_1)],test_y[test_idx_1]])\n",
    "\n",
    "\n",
    "#shuffle them\n",
    "#set seed to make the selection reproducible\n",
    "rng=np.random.RandomState(42)\n",
    "new_idx=rng.permutation(len(bal_train_y))\n",
    "bal_train_x=bal_train_x[new_idx]\n",
    "bal_train_y=bal_train_y[new_idx]\n",
    "\n",
    "new_idx=rng.permutation(len(bal_test_y))\n",
    "bal_test_x=bal_test_x[new_idx]\n",
    "bal_test_y=bal_test_y[new_idx]\n",
    "\n",
    "print np.mean(bal_train_y)\n",
    "print np.mean(bal_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D,MaxPooling2D\n",
    "\n",
    "input_dim=train_x.shape[2]\n",
    "activation='relu'\n",
    "loss='binary_crossentropy'\n",
    "optimizer='adadelta'\n",
    "init='uniform'\n",
    "pool_size=(8,1)\n",
    "window_size=5\n",
    "dense_n=64\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#Convolution layer 1\n",
    "model.add(Convolution2D(20,window_size,1, border_mode='valid',input_shape=(1,input_dim,1)))\n",
    "model.add(Activation(activation))\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "#model.add(Dropout(0.25))\n",
    "\n",
    "#Convolution layer 2\n",
    "model.add(Convolution2D(50,window_size,1, border_mode='valid'))\n",
    "model.add(Activation(activation))\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "#model.add(Dropout(0.25))\n",
    "\n",
    "#Dense layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(dense_n,activation=activation))\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "#final layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#compile model\n",
    "model.compile(loss=loss,optimizer=optimizer,class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17651 samples, validate on 4413 samples\n",
      "Epoch 1/100\n",
      "17651/17651 [==============================] - 7s - loss: 0.7190 - acc: 0.5191 - val_loss: 0.6936 - val_acc: 0.4906\n",
      "Epoch 00000: val_loss improved from inf to 0.69360, saving model to best_model\n",
      "Epoch 2/100\n",
      "17651/17651 [==============================] - 7s - loss: 0.6837 - acc: 0.5781 - val_loss: 0.6345 - val_acc: 0.6155\n",
      "Epoch 00001: val_loss improved from 0.69360 to 0.63448, saving model to best_model\n",
      "Epoch 3/100\n",
      "17651/17651 [==============================] - 7s - loss: 0.6348 - acc: 0.6581 - val_loss: 0.5850 - val_acc: 0.6868\n",
      "Epoch 00002: val_loss improved from 0.63448 to 0.58496, saving model to best_model\n",
      "Epoch 4/100\n",
      "17651/17651 [==============================] - 7s - loss: 0.5920 - acc: 0.6905 - val_loss: 0.5865 - val_acc: 0.6832\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 5/100\n",
      "17651/17651 [==============================] - 7s - loss: 0.5663 - acc: 0.7170 - val_loss: 0.5575 - val_acc: 0.7152\n",
      "Epoch 00004: val_loss improved from 0.58496 to 0.55746, saving model to best_model\n",
      "Epoch 6/100\n",
      "17651/17651 [==============================] - 7s - loss: 0.5491 - acc: 0.7226 - val_loss: 0.5950 - val_acc: 0.6805\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 7/100\n",
      "17651/17651 [==============================] - 7s - loss: 0.5391 - acc: 0.7329 - val_loss: 0.5551 - val_acc: 0.7099\n",
      "Epoch 00006: val_loss improved from 0.55746 to 0.55508, saving model to best_model\n",
      "Epoch 8/100\n",
      "17651/17651 [==============================] - 7s - loss: 0.5216 - acc: 0.7465 - val_loss: 0.5121 - val_acc: 0.7562\n",
      "Epoch 00007: val_loss improved from 0.55508 to 0.51213, saving model to best_model\n",
      "Epoch 9/100\n",
      "17651/17651 [==============================] - 7s - loss: 0.5055 - acc: 0.7579 - val_loss: 0.5023 - val_acc: 0.7496\n",
      "Epoch 00008: val_loss improved from 0.51213 to 0.50228, saving model to best_model\n",
      "Epoch 10/100\n",
      "17651/17651 [==============================] - 7s - loss: 0.4996 - acc: 0.7601 - val_loss: 0.5223 - val_acc: 0.7544\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 11/100\n",
      "17651/17651 [==============================] - 7s - loss: 0.4798 - acc: 0.7733 - val_loss: 0.4931 - val_acc: 0.7548\n",
      "Epoch 00010: val_loss improved from 0.50228 to 0.49315, saving model to best_model\n",
      "Epoch 12/100\n",
      "17651/17651 [==============================] - 7s - loss: 0.4687 - acc: 0.7775 - val_loss: 0.4949 - val_acc: 0.7664\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 13/100\n",
      "17651/17651 [==============================] - 7s - loss: 0.4548 - acc: 0.7915 - val_loss: 0.4587 - val_acc: 0.7838\n",
      "Epoch 00012: val_loss improved from 0.49315 to 0.45874, saving model to best_model\n",
      "Epoch 14/100\n",
      "17651/17651 [==============================] - 7s - loss: 0.4442 - acc: 0.7922 - val_loss: 0.5260 - val_acc: 0.7353\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 15/100\n",
      "17651/17651 [==============================] - 7s - loss: 0.4334 - acc: 0.7993 - val_loss: 0.4804 - val_acc: 0.7750\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 16/100\n",
      "17651/17651 [==============================] - 7s - loss: 0.4281 - acc: 0.8029 - val_loss: 0.4785 - val_acc: 0.7784\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 17/100\n",
      "17651/17651 [==============================] - 7s - loss: 0.4075 - acc: 0.8133 - val_loss: 0.6054 - val_acc: 0.7111\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 18/100\n",
      "17651/17651 [==============================] - 7s - loss: 0.4001 - acc: 0.8171 - val_loss: 0.4789 - val_acc: 0.7738\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 19/100\n",
      "17651/17651 [==============================] - 7s - loss: 0.3820 - acc: 0.8287 - val_loss: 0.4798 - val_acc: 0.7759\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 20/100\n",
      "17651/17651 [==============================] - 7s - loss: 0.3759 - acc: 0.8322 - val_loss: 0.4990 - val_acc: 0.7664\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 21/100\n",
      "17651/17651 [==============================] - 7s - loss: 0.3540 - acc: 0.8456 - val_loss: 0.5171 - val_acc: 0.7544\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 00020: early stopping\n",
      "22064/22064 [==============================] - 3s     \n",
      "1010/1010 [==============================] - 0s     \n",
      "train score: 0.820975344453\n",
      "test score: 0.768316831683\n",
      "It took: 158.73705101\n",
      "\n",
      "prediction balance:\n",
      "0.540609137056\n",
      "0.539603960396\n"
     ]
    }
   ],
   "source": [
    "train_pred,test_pred=fit_keras_model(\n",
    "    model,bal_train_x,bal_train_y,bal_test_x,bal_test_y,validation_split=0.2)\n",
    "\n",
    "print '\\nprediction balance:'\n",
    "print np.mean(train_pred)\n",
    "print np.mean(test_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
