{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Very naive deep learning on the vector of surrounding bases\n",
    "\n",
    "---\n",
    "\n",
    "### Data\n",
    "\n",
    "Naive feature vectors. The original sequence of validation/test and train data does not overlap! ( but train data points can overlap with train data points, and test-validation can overlap with test-validation data ) This overlapping does not lead to unintentional label leakage!\n",
    "\n",
    "\n",
    "\n",
    "### Notes\n",
    "\n",
    "\n",
    "- better than acgt\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instruct theano to use gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['THEANO_FLAGS']='device=gpu'\n",
    "\n",
    "import sys\n",
    "sys.path.append('../my_modules')\n",
    "from loading_utils import read_my_data\n",
    "\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os,subprocess\n",
    "workdir='/mnt/Data1/ribli/methylation_code/modelling'\n",
    "subprocess.call(['mkdir',workdir])\n",
    "os.chdir(workdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data... \n",
      "Loading data... \n"
     ]
    }
   ],
   "source": [
    "train_id,train_x,train_y = read_my_data(\n",
    "    fname='../prepare_data/big_train_feat_vect.csv')\n",
    "test_id,test_x,test_y = read_my_data(\n",
    "    fname='../prepare_data/big_test_feat_vect.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "annot=pd.read_csv('../explore_data/relevant_annotations.csv',sep='\\t',header=None)\n",
    "annot.columns=['id','Regulatory_Feature_Group','Relation_to_UCSC_CpG_Island',\n",
    "    'Strand','Infinium_Design_Type','Random_Loci','Methyl27_Loci']\n",
    "annot.fillna(0,inplace=True)\n",
    "train_merged=pd.DataFrame(train_id,columns=['id']).merge(annot,on=['id'])\n",
    "test_merged=pd.DataFrame(test_id,columns=['id']).merge(annot,on=['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select inidces for islands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cg_exl_idx=np.array([x[499]==2 and x[500]==3 for x in train_x])\n",
    "annot_idx=np.array(np.zeros(len(train_x)),dtype=bool)\n",
    "annot_idx[train_merged[train_merged.Relation_to_UCSC_CpG_Island=='Island'].index]=True\n",
    "train_idx=cg_exl_idx & annot_idx\n",
    "train_idx_0=cg_exl_idx & annot_idx & (train_y ==0)\n",
    "train_idx_1=cg_exl_idx & annot_idx & (train_y ==1)\n",
    "\n",
    "\n",
    "cg_exl_idx=np.array([x[499]==2 and x[500]==3 for x in test_x])\n",
    "annot_idx=np.array(np.zeros(len(test_x)),dtype=bool)\n",
    "annot_idx[test_merged[test_merged.Relation_to_UCSC_CpG_Island=='Island'].index]=True\n",
    "test_idx=cg_exl_idx & annot_idx\n",
    "test_idx_0=cg_exl_idx & annot_idx & (test_y==0)\n",
    "test_idx_1=cg_exl_idx & annot_idx & (test_y==1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape x data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make it image like\n",
    "train_x,test_x=[x.reshape((-1,1,1000,1)) for x in (train_x,test_x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "def fit_keras_model(model,train_x,train_y,test_x,test_y,validation_split=0.05):\n",
    "    start=time.time()\n",
    "    \n",
    "    #callbacks\n",
    "    best_model=ModelCheckpoint('best_model',save_best_only=True,verbose=1)\n",
    "    early_stop=EarlyStopping(patience=7,verbose=1)\n",
    "    \n",
    "    #train it\n",
    "    callb_hist=model.fit(train_x,train_y,nb_epoch = 100,\n",
    "                         show_accuracy=True,verbose=1,\n",
    "                        validation_split=validation_split,\n",
    "                        callbacks=[best_model,early_stop])\n",
    "    #predict\n",
    "    model.load_weights('best_model')\n",
    "    train_pred=model.predict_classes(train_x).ravel()\n",
    "    test_pred=model.predict_classes(test_x).ravel()\n",
    "    \n",
    "    train_pred_pr=model.predict(train_x).ravel()\n",
    "    test_pred_pr=model.predict(test_x).ravel()\n",
    "\n",
    "    #check errors\n",
    "    print 'train accuracy:',list((train_pred==train_y)).count(True)/float(len(train_y))\n",
    "    print 'test accuracy:',list((test_pred==test_y)).count(True)/float(len(test_y))\n",
    "\n",
    "    print 'It took:',time.time()-start    \n",
    "    return train_pred,test_pred,train_pred_pr,test_pred_pr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "bal_train_x=np.concatenate([train_x[train_idx_0][:np.sum(train_idx_1)],train_x[train_idx_1]])\n",
    "bal_train_y=np.concatenate([train_y[train_idx_0][:np.sum(train_idx_1)],train_y[train_idx_1]])\n",
    "\n",
    "bal_test_x=np.concatenate([test_x[test_idx_0][:np.sum(test_idx_1)],test_x[test_idx_1]])\n",
    "bal_test_y=np.concatenate([test_y[test_idx_0][:np.sum(test_idx_1)],test_y[test_idx_1]])\n",
    "\n",
    "\n",
    "#shuffle them\n",
    "#set seed to make the selection reproducible\n",
    "rng=np.random.RandomState(42)\n",
    "new_idx=rng.permutation(len(bal_train_y))\n",
    "bal_train_x=bal_train_x[new_idx]\n",
    "bal_train_y=bal_train_y[new_idx]\n",
    "\n",
    "new_idx=rng.permutation(len(bal_test_y))\n",
    "bal_test_x=bal_test_x[new_idx]\n",
    "bal_test_y=bal_test_y[new_idx]\n",
    "\n",
    "print np.mean(bal_train_y)\n",
    "print np.mean(bal_test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make acgt to atgc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bal_train_x[bal_train_x==4]=5\n",
    "bal_train_x[bal_train_x==2]=4\n",
    "bal_train_x[bal_train_x==5]=2\n",
    "\n",
    "bal_test_x[bal_test_x==4]=5\n",
    "bal_test_x[bal_test_x==2]=4\n",
    "bal_test_x[bal_test_x==5]=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D,MaxPooling2D\n",
    "\n",
    "input_dim=train_x.shape[2]\n",
    "activation='relu'\n",
    "loss='binary_crossentropy'\n",
    "optimizer='adadelta'\n",
    "init='uniform'\n",
    "pool_size=(8,1)\n",
    "window_size=5\n",
    "dense_n=64\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#Convolution layer 1\n",
    "model.add(Convolution2D(20,window_size,1, border_mode='valid',input_shape=(1,input_dim,1)))\n",
    "model.add(Activation(activation))\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "#model.add(Dropout(0.25))\n",
    "\n",
    "#Convolution layer 2\n",
    "model.add(Convolution2D(50,window_size,1, border_mode='valid'))\n",
    "model.add(Activation(activation))\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "#model.add(Dropout(0.25))\n",
    "\n",
    "#Dense layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(dense_n,activation=activation))\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "#final layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#compile model\n",
    "model.compile(loss=loss,optimizer=optimizer,class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17651 samples, validate on 4413 samples\n",
      "Epoch 1/100\n",
      "17651/17651 [==============================] - 7s - loss: 0.7181 - acc: 0.5486 - val_loss: 0.6776 - val_acc: 0.5035\n",
      "Epoch 00000: val_loss improved from inf to 0.67756, saving model to best_model\n",
      "Epoch 2/100\n",
      "17651/17651 [==============================] - 7s - loss: 0.6592 - acc: 0.6195 - val_loss: 0.6095 - val_acc: 0.6886\n",
      "Epoch 00001: val_loss improved from 0.67756 to 0.60951, saving model to best_model\n",
      "Epoch 3/100\n",
      "17651/17651 [==============================] - 7s - loss: 0.6201 - acc: 0.6650 - val_loss: 0.9247 - val_acc: 0.5012\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 4/100\n",
      "17651/17651 [==============================] - 7s - loss: 0.5623 - acc: 0.7163 - val_loss: 0.5158 - val_acc: 0.7467\n",
      "Epoch 00003: val_loss improved from 0.60951 to 0.51580, saving model to best_model\n",
      "Epoch 5/100\n",
      "17651/17651 [==============================] - 7s - loss: 0.5293 - acc: 0.7333 - val_loss: 0.5241 - val_acc: 0.7292\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 6/100\n",
      "17651/17651 [==============================] - 7s - loss: 0.5110 - acc: 0.7529 - val_loss: 0.4575 - val_acc: 0.7859\n",
      "Epoch 00005: val_loss improved from 0.51580 to 0.45745, saving model to best_model\n",
      "Epoch 7/100\n",
      "17651/17651 [==============================] - 7s - loss: 0.4912 - acc: 0.7609 - val_loss: 0.7410 - val_acc: 0.5690\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 8/100\n",
      "17651/17651 [==============================] - 7s - loss: 0.4825 - acc: 0.7692 - val_loss: 0.4277 - val_acc: 0.8069\n",
      "Epoch 00007: val_loss improved from 0.45745 to 0.42769, saving model to best_model\n",
      "Epoch 9/100\n",
      "17651/17651 [==============================] - 7s - loss: 0.4775 - acc: 0.7743 - val_loss: 0.5041 - val_acc: 0.7541\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 10/100\n",
      "17651/17651 [==============================] - 7s - loss: 0.4476 - acc: 0.7906 - val_loss: 0.4361 - val_acc: 0.8044\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 11/100\n",
      "17651/17651 [==============================] - 7s - loss: 0.4467 - acc: 0.7921 - val_loss: 0.4658 - val_acc: 0.7686\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 12/100\n",
      "17651/17651 [==============================] - 7s - loss: 0.4412 - acc: 0.7934 - val_loss: 0.4284 - val_acc: 0.7979\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 13/100\n",
      "17651/17651 [==============================] - 7s - loss: 0.4360 - acc: 0.7946 - val_loss: 0.8354 - val_acc: 0.6701\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 14/100\n",
      "17651/17651 [==============================] - 7s - loss: 0.4316 - acc: 0.8013 - val_loss: 0.4021 - val_acc: 0.8221\n",
      "Epoch 00013: val_loss improved from 0.42769 to 0.40213, saving model to best_model\n",
      "Epoch 15/100\n",
      "17651/17651 [==============================] - 7s - loss: 0.4194 - acc: 0.8081 - val_loss: 0.6078 - val_acc: 0.6605\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 16/100\n",
      "17651/17651 [==============================] - 7s - loss: 0.4083 - acc: 0.8127 - val_loss: 0.3977 - val_acc: 0.8278\n",
      "Epoch 00015: val_loss improved from 0.40213 to 0.39767, saving model to best_model\n",
      "Epoch 17/100\n",
      "17651/17651 [==============================] - 7s - loss: 0.4096 - acc: 0.8121 - val_loss: 0.6496 - val_acc: 0.6719\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 18/100\n",
      "17651/17651 [==============================] - 7s - loss: 0.4050 - acc: 0.8142 - val_loss: 0.4330 - val_acc: 0.8103\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 19/100\n",
      "17651/17651 [==============================] - 7s - loss: 0.3838 - acc: 0.8245 - val_loss: 0.4523 - val_acc: 0.7902\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 20/100\n",
      "17651/17651 [==============================] - 7s - loss: 0.3626 - acc: 0.8372 - val_loss: 0.4209 - val_acc: 0.8171\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 21/100\n",
      "17651/17651 [==============================] - 7s - loss: 0.3642 - acc: 0.8372 - val_loss: 0.4529 - val_acc: 0.7963\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 22/100\n",
      "17651/17651 [==============================] - 7s - loss: 0.3609 - acc: 0.8362 - val_loss: 0.4160 - val_acc: 0.8099\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 23/100\n",
      "17651/17651 [==============================] - 7s - loss: 0.3501 - acc: 0.8447 - val_loss: 0.4296 - val_acc: 0.8090\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 24/100\n",
      "17651/17651 [==============================] - 7s - loss: 0.3340 - acc: 0.8555 - val_loss: 0.4887 - val_acc: 0.7779\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 00023: early stopping\n",
      "22064/22064 [==============================] - 3s     \n",
      "1010/1010 [==============================] - 0s     \n",
      "train accuracy: 0.847625090645\n",
      "test accuracy: 0.819801980198\n",
      "It took: 184.965501785\n",
      "\n",
      "prediction balance:\n",
      "0.524655547498\n",
      "0.523762376238\n"
     ]
    }
   ],
   "source": [
    "train_pred,test_pred,train_pred_pr,test_pred_pr=fit_keras_model(\n",
    "    model,bal_train_x,bal_train_y,bal_test_x,bal_test_y,validation_split=0.2)\n",
    "\n",
    "print '\\nprediction balance:'\n",
    "print np.mean(train_pred)\n",
    "print np.mean(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import metrics\n",
    "def plot_roc(y,probs):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y,probs)\n",
    "    auc=metrics.roc_auc_score(y,probs)\n",
    "    \n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.plot(fpr,tpr,lw=2)\n",
    "    plt.plot([0,1],[0,1],lw=2)\n",
    "    plt.xlim(-0.01,1.01)\n",
    "    plt.ylim(-0.01,1.01)\n",
    "    plt.xlabel('FP rate')\n",
    "    plt.ylabel('TP rate')\n",
    "    print 'auc:',auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc: 0.900970493089\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAGCCAYAAAAR7+9DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJI1kLCRJIxkLSdJIxkKSNJKxkCSN9P8BtGG1paXgthQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f24d9267e90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_roc(bal_test_y,test_pred_pr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
